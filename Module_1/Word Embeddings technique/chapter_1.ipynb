{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f64f69",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a76249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 17:36:26.622302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-29 17:36:26.641953: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-29 17:36:26.647851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-29 17:36:26.712691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 5], [6, 1, 5]]\n",
      "{'ate': 1, 'apples': 2, 'bob': 3, 'and': 4, 'pears': 5, 'fred': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "new_texts = ['bob ate pears', 'fred ate pears']\n",
    "print(tokenizer.texts_to_sequences(new_texts))\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139ced0",
   "metadata": {},
   "source": [
    "## Tokenizer parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a9be4",
   "metadata": {},
   "source": [
    "The Tokenizer object can be initialized with a number of optional parameters. By default, the Tokenizer filters out any punctuation and white space. You can specify custom filtering with the filters parameter. The parameter takes in a string, where each character in the string is filtered out.\n",
    "\n",
    "When a new text contains words not in the corpus vocabulary, those words are known as out-of-vocabulary (OOV) words. The texts_to_sequences automatically filters out all OOV words. However, if we want to specify each OOV word with a special vocabulary token (e.g. 'OOV'), we can initialize the Tokenizer with the oov_token parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4bae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1]]\n",
      "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "  oov_token='OOV')\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "print(tokenizer.texts_to_sequences(['bob ate bacon']))\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54599d99",
   "metadata": {},
   "source": [
    "The num_words parameter lets us specify the maximum number of vocabulary words to use. For example, if we set num_words=100 when initializing the Tokenizer, it will only use the 100 most frequent words in the vocabulary and filter out the remaining vocabulary words. This can be useful when the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cac7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2)\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "# the two most common words are 'ate' and 'apples'\n",
    "# the tokenizer will filter out all other words\n",
    "# for the sentence 'bob ate pears', only 'ate' will be kept\n",
    "# since 'ate' maps to an integer ID of 1, the only value \n",
    "# in the token sequence will be 1\n",
    "print(tokenizer.texts_to_sequences(['bob ate pears']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc545385",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "* **Embeddings** : C'est une représentation vectorielle des mots ou des phrases qui capturent leurs relations semantiques. Les embeddings sont utilisés dans de nombreuses taches NLP, notamment pour les taches de classification, de retraitement et de generation de textes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Utilisation de l'objet Tokenizer (TensorFlow/Keras)\n",
    "\n",
    "L'outil principal utilisé est la classe `tf.keras.preprocessing.text.Tokenizer`. Elle automatise plusieurs étapes cruciales :\n",
    "\n",
    "* **Indexation** : Chaque mot du vocabulaire est associé à un identifiant entier unique, attribué selon la fréquence d'apparition (les mots les plus fréquents ont les index les plus bas).\n",
    "* **fit_on_texts** : Cette méthode analyse le corpus pour créer le dictionnaire interne (le vocabulaire).\n",
    "* **texts_to_sequences** : Cette méthode transforme une liste de textes en listes de nombres (vecteurs), remplaçant chaque mot par son nombre correspondant.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Paramètres et Gestion des Exceptions\n",
    "\n",
    "Le `Tokenizer` offre des options pour affiner le traitement des données :\n",
    "\n",
    "* **Filtrage** : Par défaut, la ponctuation et les majuscules sont supprimées pour normaliser le texte.\n",
    "* **OOV (Out-Of-Vocabulary)** : Lorsqu'un nouveau texte contient un mot absent du vocabulaire initial, il est normalement ignoré. Le paramètre `oov_token` permet de remplacer ces mots inconnus par un jeton spécial (ex: \"OOV\") afin de conserver la structure de la phrase.\n",
    "* **num_words** : Ce paramètre limite la taille du vocabulaire aux $N$ mots les plus fréquents. C'est essentiel pour réduire la complexité du modèle et éviter le surapprentissage sur des mots rares.\n",
    "\n",
    "---\n",
    "\n",
    "# Implémentation : La fonction `tokenize_text_corpus`\n",
    "\n",
    "Voici le code complété et l'explication technique du processus de transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176c712",
   "metadata": {},
   "source": [
    "the exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_window_indices(sequence, target_index, half_window_size):\n",
    "    left_incl=max(0,(target_index-half_window_size))\n",
    "    right_excl=min(len(sequence),(target_index+half_window_size+1))\n",
    "    return [left_incl,right_excl]\n",
    "\n",
    "def get_target_and_size(sequence, target_index, window_size):\n",
    "    target_word=sequence[target_index]\n",
    "    half_window_size=window_size // 2\n",
    "    return target_word , half_window_size\n",
    "\n",
    "# Skip-gram embedding model\n",
    "class EmbeddingModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    # Convert a list of text strings into word sequences\n",
    "    def get_target_and_context(self, sequence, target_index, window_size):\n",
    "        target_word, half_window_size = get_target_and_size(\n",
    "            sequence, target_index, window_size\n",
    "        )\n",
    "        left_incl, right_excl = get_window_indices(\n",
    "            sequence, target_index, half_window_size)\n",
    "        return target_word, left_incl, right_excl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cceecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7567f31",
   "metadata": {},
   "source": [
    "skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e675024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class EmbeddingModel(object):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    def tokenize_text_corpus(self, texts):\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        return sequences\n",
    "\n",
    "    def get_target_and_context(self, sequence, target_index, window_size):\n",
    "        target_word = sequence[target_index]\n",
    "        half_window_size = window_size // 2\n",
    "        left_incl = max(0, target_index - half_window_size)\n",
    "        right_excl = min(len(sequence), target_index + half_window_size + 1)\n",
    "        return target_word, left_incl, right_excl\n",
    "    \n",
    "    def create_target_context_pairs(self, texts, window_size):\n",
    "        pairs = []\n",
    "        # 1. Conversion du texte en séquences numériques\n",
    "        sequences = self.tokenize_text_corpus(texts)      \n",
    "        \n",
    "        for sequence in sequences:\n",
    "            for i in range(len(sequence)):\n",
    "                # 2. Récupération des limites de la fenêtre pour chaque mot cible\n",
    "                target_word, left_incl, right_excl = self.get_target_and_context(\n",
    "                    sequence, i, window_size)\n",
    "                \n",
    "                # 3. Création des paires (Target, Context)\n",
    "                for j in range(left_incl, right_excl):\n",
    "                    # On s'assure de ne pas créer une paire avec le mot cible lui-même\n",
    "                    if j != i:\n",
    "                        pairs.append((target_word, sequence[j]))\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e62186",
   "metadata": {},
   "source": [
    "Embedding lookup\n",
    "When training the embedding model, the \"forward\" run consists of variable initialization/retrieval followed by embedding lookup for the current iteration's training batch. Embedding lookup refers to retrieving the embedding vectors for each word ID in the training batch. Since the embedding matrix's rows are each unique embedding vectors, we perform the lookup simply by retrieving the rows corresponding to the training batch's word IDs.\n",
    "\n",
    "The function we use to retrieve the embedding vectors is tf.nn.embedding_lookup. It takes in two required arguments, which are the embedding matrix variable and vocabulary IDs to lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5eb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(object):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def forward(self, target_ids):\n",
    "        # 1. Préparation de l'initialiseur\n",
    "        initializer = get_initializer(self.embedding_dim, self.vocab_size)\n",
    "        \n",
    "        # 2. Création ou récupération de la matrice (Variable TensorFlow)\n",
    "        self.embedding_matrix = tf.compat.v1.get_variable(\n",
    "            'embedding_matrix', \n",
    "            initializer=initializer\n",
    "        )\n",
    "        \n",
    "        # 3. Extraction des vecteurs d'embedding pour les IDs cibles\n",
    "        embeddings = tf.nn.embedding_lookup(self.embedding_matrix, target_ids)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44603cf6",
   "metadata": {},
   "source": [
    "to calculate the loss for our candidate sampling algorithm, we need to create weight and bias variables. The weight variable will have shape [self.vocab_size, self.embedding_dim], while the bias variable will have shape [self.vocab_size].\n",
    "\n",
    "We'll initialize the values for both the weight and bias variables to all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Skip-gram embedding model\n",
    "class EmbeddingModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    # Get bias and weights for calculating loss\n",
    "    def get_bias_weights(self):\n",
    "        weights_initializer = tf.zeros([self.vocab_size, self.embedding_dim])\n",
    "        bias_initializer = tf.zeros([self.vocab_size])\n",
    "        weights = tf.compat.v1.get_variable('weights',\n",
    "            initializer=weights_initializer)\n",
    "        bias = tf.compat.v1.get_variable('bias',\n",
    "            initializer=bias_initializer)\n",
    "        return weights, bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bed9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
