{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f64f69",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a76249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 17:36:26.622302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-29 17:36:26.641953: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-29 17:36:26.647851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-29 17:36:26.712691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 5], [6, 1, 5]]\n",
      "{'ate': 1, 'apples': 2, 'bob': 3, 'and': 4, 'pears': 5, 'fred': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "new_texts = ['bob ate pears', 'fred ate pears']\n",
    "print(tokenizer.texts_to_sequences(new_texts))\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139ced0",
   "metadata": {},
   "source": [
    "## Tokenizer parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a9be4",
   "metadata": {},
   "source": [
    "The Tokenizer object can be initialized with a number of optional parameters. By default, the Tokenizer filters out any punctuation and white space. You can specify custom filtering with the filters parameter. The parameter takes in a string, where each character in the string is filtered out.\n",
    "\n",
    "When a new text contains words not in the corpus vocabulary, those words are known as out-of-vocabulary (OOV) words. The texts_to_sequences automatically filters out all OOV words. However, if we want to specify each OOV word with a special vocabulary token (e.g. 'OOV'), we can initialize the Tokenizer with the oov_token parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4bae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1]]\n",
      "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "  oov_token='OOV')\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "print(tokenizer.texts_to_sequences(['bob ate bacon']))\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54599d99",
   "metadata": {},
   "source": [
    "The num_words parameter lets us specify the maximum number of vocabulary words to use. For example, if we set num_words=100 when initializing the Tokenizer, it will only use the 100 most frequent words in the vocabulary and filter out the remaining vocabulary words. This can be useful when the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cac7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2)\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "# the two most common words are 'ate' and 'apples'\n",
    "# the tokenizer will filter out all other words\n",
    "# for the sentence 'bob ate pears', only 'ate' will be kept\n",
    "# since 'ate' maps to an integer ID of 1, the only value \n",
    "# in the token sequence will be 1\n",
    "print(tokenizer.texts_to_sequences(['bob ate pears']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397072f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
